\subsection{Preserving statistical validity in adaptive data analysis}

Analyze data and see whether the conclusions generalize to the whole dataset.

How do you analyze questions such as, does student nutrition affect academic performance?

Pick candidate foods; fit a linear function of 3 selected foods. Obtain linear correlation. Obtain a number which is the significance score: the probability of seeing such an outcome if there is no relationship. Here it is $<10^{-5}$. You are now an expert on nutrition, can publish books!

Actually the datset you got was generated by 50 uncorrelated gaussians.

What went wrong? 

Taking a new sample, you'll against

You're not supposed to use the same dataset to select variables and to test regression.

Freedman's Paradox: such paradoxes can distort the significance levels of conventional statistical tests.

Disconnect between ways they're analyzed and used in practice.

Put in fresh data, put through a procedure (hypothesis tests, regression, learning), give result and statistical guarantees ($p$-values, confidence intervals, prediction intervals). 

Data analysis is adaptive:
\begin{enumerate}
\item
exploratory data analysis
\item 
variable selection
\item
hyper-parameter tuning
\item
shared data - findings inform others
\end{enumerate}
One analyst influences the analyses another analyst is doing. 

Start with some fresh data set; analyze it, then perform another analysis based on the first output. The procedure is not independent of the data.

Is this a real problem, or just a theoretical issue? 

There is a growing recognition that many published 

``Why most published research findings are false," Ioannidis, 2005. ``Irreproducible preclinical research exceeds 50\%, resulting in approximately US\$28B/year loss." Freedman, Cockburn, Simcoe 15. 

How Science Goes Wrong, The Economist.

One significant cause is adaptive data analysis. 

Rare and impractical to make decision s beforehand. Search for a combination athat yields statistical significance, and to report only what worked.

Simple, concrete problem which captures what arises very well. 

Evaluating adaptive queries: 
Want $\phi_i:X\to [0,1]$, want $|v_i-\E_{x\sim P} \phi_i(x)|\le \tau$ with probability $1-\be$. 
In context of ML, Kearns: Statistical query oracle.

How many samples will the algorithm need. 

A lot of data analysis can be implemented in this model.

Answering non-adaptive SQ's: Given $m$ non-adaptive query functions $\phi_i$ and $n$ iid samples from $P$ estimate $\E_{x\sim P}[\phi_i(x)]$. Use empirical mean; get Chernoff bound.

Answering adaptive SQ's: estimate expectations of $m$ adaptively chosen functions. What if we use $\E_S[\phi_i]$? For some constant $\be>0$, $\tau >0$, $n\ge m$.

We can do the safe choice, $n=O\pf{m\ln (m/be)}{\tau^2}$. 

There exists an algoorithm that can answer $t$ adaptively chosen SQ's with accuracy $\tau$ for 
\[
n_1 = O\pa{
\fc{\ln m}{\tau^2}\fc{\sqrt{\ln m\ln |X|}}{\tau^{1.5}}
}
\]
in times $O(n_1\ln|X|)$.

Tool: differential privcy.
Look at the outcome of algorithm that's adjacent. 

A randomized algorithm $A$ is $(\ep,\de)$-differentially private if for any $S,S'$ such that $\De(S,S')=1$, 
\[
\Pj_A(A(S)\in Z)\le e^\ep \Pj_A(A(S')\in Z)+\de.
\]

Why DP? DP composes adaptively. Combine $(\ep_i,\de_i)$ DP algorithms gives $(\sum\ep_i, \sum\de_i)$. In fact composition of $m$ $\ep$-DP algorithms: for every $\de>0$ is $(\ep\sqrt{2m\ln\prc{\de}},\de)$-DP.  (Square root scaling)

If $\tau$-DP algorithm $A$ outputs $\phi:X\to [0,1]$ then
\[
\Pj_{A,S\sim P^n}[|\E_S[A(S)]-\E_P[A(S)]|\ge \tau]\le 6e^{-\tau^2n}.
\]
%in order to violate chernoff eed to learn something about data set.

Back to queries.
If DP $A$ outputs function then $\E_S[A(S)]\approx \E_P[A(S)]$ with high probablity.

Make analyst DP by answering queries with DP
Answer queries for $\E_S[A(S)]$ with DP by counting queries.

Further developments.
\begin{enumerate}
\item
$n=\Om\pf{\sqrt m}{\tau}$ for polytime algorithms assuming OWF exists.
\item
$n=O(\sqrt m\ln m/\tau^2)$. 
\item 
Stronger tight generalizations for DP algoirthms
\item
General adaptive setting and other techniques: description lenght and max-information.
\item Application: algorithms for reusing holdout set in ML
\item
Application to maintaining accurate leaderboard in ML competitions. 
\end{enumerate}
TCS+ talk Aaron roth on YouTube.


%

\subsection{Dimensionality reduction for $k$-means clustering}
Dimensionality reduction: replace large high-D dataset with sketch.

Soluion on $\ol A$ should approximate original solution.

Use as preprocessing step for $k$-means and PCA, get simultaneous runtime improvement.

Review $k$-means clustering: Choose $k$ clusters to minimize intra-cluster variance. 

\[\sum_{i=1}^n \ve{a_i-\mu(C[a_i])}_2^2.\]
$k$-means ++ initialization with Lloyd's heuristic, $k$-means algorithm, $O(\ln k)$ approximation guaranteeed, typically performs much better. Use dimensionality reduction.

%generic speedup.

$k$-means clustering is low-rank approximation.

Replace every row of matrix with corresponding cluster mean. Only $k$ unique rows, so rank $k$. error $\ve{A-C(A)}_F$ 

$C(A)$ is actually a projection of $A$'s columns onto a rank $k$ subspace. %orth disj support.
$X_CX_C^T A = C(A)$.

Rewrite objective function:
\[
\min_{\rank(X)=k,X\in S} \ve{A-XX^TA}_F^2.
\]
$X$ is rank $k$ orthonormal matrix and for $k$-means $S$ is set of clustering indices.

We can solve anyproblem like $\min_{\rank(X)=k,X\in S}\ve{A-XX^TA}_F^2.$ if for all rank $k$ $X$, $\approx \ve{\wt A-\cdots}$. 

Projection-cost preserving sketch.

Specifically we want for all $X$, $\ve{\wt A-XX^T \wt A}_F^2 + c=(1\pm \ep)\ve{A-XX^TA}_F^2$. If we find $\ga$-approximate solution, get get out $\ga(1+\ep)$.

This is similar to the coresets of Feldman, Schmidt, Sohler 2013.

$k$-means clustering is just constrained $k$-rank approximation. 
We cna construct projection-cose prserving sketch $\wt A$ that approximates the distance from $A$ to any rank $k$ subspce. Sronger guarantee than has been sought in prior work on approximate PCS via sketching.

Can reduce to $\approx k$ and give $1+\ep$ projection-cost preservation. Useful for others? 
Go through different methods. Do analysis for SVD.

Reduce $A$ by projecting onto top $k/\ep$ right SV's. Improves on prior work by $\ep$. Robust to fairly robust SVD.

No constant factors on $k/\ep$, typically fewer dimensions are required.
Faster dim-reduction techniques. 

Johnson-Lindenstrauss random projection matrix. 
%making $\Pi$ sparse to speed up
Can get constant factor with $\ln k$ dimensions! First sketch with dimension sublinear in $k$.

Sketch is data oblivious: Useful in distributed applications.
\begin{enumerate}
\item
Lowest communication distributed $k$-means.
\item Streaming PCA in single pass.
\end{enumerate}

Standard sketches for low-rank approximations: $\spn(\wt A)$ contains a good lw rank approximation for $A$, but we must return to .. to find it.

Is $\ep$ dependence necessary?

First single shot sampling techniques...

Analysis for SVD $A=U\Si V^T$.
%same as PCA if we mean center $A$'s rows first
Partial SVD: $\ve{A-A_m}_F^2 = \min_{m} \ve{A-XX^TA}_F^2$. Mult by orthon doesn't change F norm.
Clm: $\ve{A_{k/\ep}-XX^TA_{}}_F^2+c = (1\pm \ep)\ve{A-XX^T A}_F^2$. SPlit $A=A_{k/\ep}+A_{\be k/\ep}$. 
Projection cost explained by sum of costs. Ignoring tail term is fine. $\bs k/\ep$ is at most $\ep$ times. 
\[
\ve{XX^TA_{\bs k/\ep}}_F^2 \le \ep\ve{A-A_k}_F^2.
\]
Analysis is very worst-case.

All proofs have similar flavor: divide into 2 components, except we have to wrory about cross terms. 
We have some hope of approximating the tail.

We rely on standard sketching tools: approximate matrix multiplication, subspace embeddings. 

Work super well in practice. Significantly improve running time. 


\subsection{Improved noisy population recovery, and reverse Bonami-Beckner inequality}

Results from the survey don't reflect exact preferences because of noise in the answers. Can we recover the answers?

\begin{mdl}
Let $\pi$ be unknown distribution in $B^n$ with support $k$. Sample string $\sim \pi$, flip coordinate with prob $\my$> Recover $\pi$ with $L^{\iy}$ distance $\ep$.
\end{mdl}
Motivations
\begin{enumerate}
\item
mixtures of product binomial distributions. let $\pi_i$ be product distributions over $B^n$ with weights $c_i$. Can  we learn $\pi_i$ and $c_i$ from the mixture.
\end{enumerate}•

Sample $x_j\sim \pi$. Generalize model of noisy pop recovery: flip $(x_j)_i$ with probability $\pi_{i,j}$. For all $i,j, \mu_{i,j}\le \mu < \rc2$.

Population recovery: for all $\mu_{i,j}=\mu$.

Previous work: restriction access, lossy disribution, noisy distribution, list recovery, genreal mixture.

This work: noisy distribution can be recovered in time $\poly(k^{\ln\ln k},n,\rc{\ep})$. For any $\mu>0$ there exists algorithm fo rnoisy recovery...

Let 
\[T_\mu f= \E_{e=N_\mu(x)}f(x+e).\]
%(1+\mu)/2
Necessary conditions: $\ve{T_\mu \pi_1-T_\mu \pi_2}_1\le \De(k,\pi_1,\pi_2)$, then any aglorithm needs $\Om\prc{\De}$ samples.

Sufficient condition! If $\ve{T_\mu f}_1\ge \De(k,\pi_1,\pi_2)\ve{f}$, noisy distribution can be recovered in $\poly(\rc{\De},n)$.
Convex opt problem, sovle efficiently by MLE.

Information theoretic version. $|\Supp(f)|=k$. Then
\[
\ve{T_\mu f}_1\ge k^{-O(\ln\ln k+\ln \rc{\mu})}\ve{f}_1
\]
$\ve{T_\mu f}_1\ge \mu^{|S|} |\wh f(S)|$.

Generalize! $g(x)=f(x)\Pj_{e\sim D_\mu}[x+e\in E]$, $\ve{T_\mu f}_1\ge \mu^{|S|} |\wh g(S)|$.

%noise samples. 
%only keep samples in $E$.

Choose $E=\set{y\in B^n}{d(x_1,y)<d(x_i,y),\text{ for all }x_i,d(x_1,x_i)\ge \ln k/\mu^2}$. Informal claim: by truncating, $g$ looks like dist on $B(x_1,\ln k/\mu^2)$.

$\Supp(f)\subeq B(n,r)$ of size $k$. 
There exists $S\subeq [n]$, $|S|\le \ln k$ such that 
\[
|\wh f(S)|\ge k^{-\ln (4r)}\ve{f}_1.
\]
Proof idea: poly $p$ of degree $\le \ln k$, $p(x)=f(x)$ for all $x\in \Supp(f)$. $\sum_S|\wh p(S)| \le kr^{\ln k}\ve{f}_1$.

recover $\poly(k^{\ln\ln k}, n,\rc{\ep})$ time.

Open
\begin{enumerate}
\item
Recover in time $\poly(k,n,\rc{\ep})$.
%current reduction not smart enough
\item
Can we recover the noisy distribution without knowing $\mu$?
\item
Can we learn mixtures of product distributions in $\poly(2^k,n,\rc{\ep})$?
\end{enumerate}•


\subsection{PRGs for spherical caps}
Prob method gives $2\ln (n/\ep)$. We get $O(\ln n + \ln \prc{\ep}\ln\ln\prc{\ep})$. 

Similar for spherical Gaussian.
\begin{enumerate}
\item
Iterated dimension reduction. 

Strong quantitative bounds for truncated moment problem for mixtures of smooth random variables. \blu{How many moments of a random variable must we match to approximate it?}
\item
pseudorandom projection matrices
\end{enumerate}

Goal: generator $G$ such that $w\in \R^n$, $x\sim \bS^{n-1}$, $b\sim B^r$, $\an{w,x}\sim_\ep^{CDF} \an{w,G(y)}$.

Random projection of $w$ onto dimension 1.

2 step dim reduction. Project $Q_1:\R^n\to \R^{\sqrt n}$. $x_1\sim \bS^{\sqrt n-1}$. Derandomize the first step. Use a pseudorandom projection matrix $P_1$. $P_1$ pseudorandom projection can be sampled in $\sim \lg n$ bits. Reduction to $\sqrt n$ dimensional problem. $n\to n^{\rc 2}\to n^{\rc{2^2}}\cdots$. $x_t\sim \bS^{\sim \lg n}$.

If we have $P_i$< we can sampl using $\sim \lg n$ random bits.

2. Two-step generator

Need $\an{Qw,x_1}\sim_\ep^{CDF}$. Make have approximately equal lower order moments. Mtach enough lower moments. How many moments to match?

3. Moment matching.

Truncated moment problem: Suppose for all $d\le r$, $\E(X^r)=E(Y^r)$, how large is DCDF$(X,Y)$?

$r=\prc{\ep}^C$ for $\ep$ CDF distance bound.

Main observation: very general, holds for all rv with matching moments. Understand structure of rv's better!

Only $\ve{Qw}_2$ matter by rotation invariance. $M=\ve{Qw}_2,N=\ve{Pq}_2$> ``Mixtures of different scalings of $z_1$. 

For random variables of the form $MZ, NZ$, 
\begin{enumerate}
\item
$\E M^d\le \E N^d$
\item
Moments of $M$ don't grow too fast
\item
$Z$ sym, $C^\iy$ CDF. 
\end{enumerate}
\blu{Matching $\sim \fc{\ln\prc{\ep}}{\ln n}$ moments enough.} Cf. structure of rv and moment matching : Anandkumar, Kakade, Hsu.

Approximate orthogonal designs. $t$-wise independence fools degree $t$ polynomials on the hypercube.

Orthogonal $t$-design fools degree $t$ polynomial on ...

Orthogonal $t$ design fools degree $t$ polys on rotation matrices. $|\E_Dp-\E_{\cal H} p|\le \ep$. Real analogues of unitary designs studies in quantum computing. 

Using Brandao-Harrow, Horodecki 12, based on Bourgain-Gamburd12, expander walks on Lie groups.

Draw $R$ according to $D$, $P=$first $\sqrt n$ rows of $R$ 
Uniform random projection $Q$ choosing first $\sqrt n$ rows of rotation matrix $R$. Even moments of $\ve{Qw}_2$ are polys in entries of $S$. 

%Stronger bounds for trunc mom


Degree 2 on sphere? Require many more ideas, because  here we're taking threshold. Rotation invariant. 

%where $\ln\ln\prc{\ep}$ in each step used $\ln\prc{\ep}$. As dim reduces, seed length rises, tradeoff. 
%spend bigger seed length for same.
%Gaussians. 
%inverse cdf.
%discretize $[0,1]$nd use tricks for $t$-wise over field. $n$ dimensions, lose exponential. 

\subsection{Rectangles are nonnegative juntas}

Alice and Bob have $x,y\in B^n$ and want to compute $F(x,y)$ minimizing communication.

We want to understand communication complexity  of $f\circ g^n$, $g$ a 2-party function (gadget), $B^b\times B^b\to B$, $x,y\in (B^b)^n$.  Inputs $x,y$ encode $z=g^n(x,y)$. %exponentially many encoding.

Intuition, inputs to outer function hidden by gadgets. Spend communication to query. CC of converse function explained by query complexity of outer. 
Conjecture: simulate cost $d$ rand protocol for $f\circ g^n$ using height $d$ rand dt for $f$. Gadget must be chosen carefully.
\[
BPP^{cc}(f\circ g^n)=BPP^{dt}(f).
\]
We prove it for degree $d$ conical junta for $f$.

\begin{df}
Conical $d$-junta: nonnegative combination of $d$-conjunctions ($\sum a_iz_{i_1}\cdots z_{i_d}$). From DT, take the sum over accepting paths.
%gen of randomized decision trees.
\end{df}


Junta theorem. $f$ partial fnction, $g$ inner-product on $\Te(\lg n)$ bits, $\Pi$ is cost $d$ rand prot for $f\circ g^n$. Then exists conical $d$ junta approximates probability of acceptance.
For each $z\in B^n$, associate 2-party encoding. 
\[
h\approx \Pj_{(x,y)\sim (g^n)^{-1}(z)}\Pi(x,y)\text{ accepts}
\]

Gadgets hide info from protocols.
Polynomial approximation. Sherali-Adams vs. LP's.

Picture. communication matrix of $f\circ g^n$. Look at cc of gadgets, $g^n$. Partitioned into bands. Understand $\Pj$ accept. induces partition of communication matrix. Suffices to understand for single rectangle. What's the fraction inside the rectangle. Main theorem: exists conical $d$-junta $h$, $\Pj[(x,y)\in R]\approx h(z)$.

Corollaries: simulation theorems.

Communication-to-query simulation for NP.
\[
\NP^{cc}(f\circ g^n)=NP^{dt}(f)\Te(b=\Te(\lg n)).
\]
%conical $d$ junta as $d$-DNF.
NP, QPP (smooth rectangle approx rank${}_+$), SBP (corruption), PostBPP communication analogues.
P, PP. BPP, MA?

Resolve open problems: query lower bound gives communication lower bound. SBP${}^{cc}$ not closed under $\cap$. small bounded-error comp: yes accept $\ge \al$, no accepted with $\le \fc{\al}{2}$. (2) corruption does not characterize $MA^{cc}$, $\nsubeq SBP^{cc}$. %prove lower bound for stronger class. 
No efficient error amplification for $\ep$-rank${}_+$. Clique vs. independent set problemL coNP${}^{cc}\gg$...

Open
\begin{enumerate}
\item
more applications of junta theorem
\item
simulation theorems for BPP
\item
improve gadget size down to $b=O(1)$. Would give new proof of $\Om(n)$ bound for set-disjointness.
\end{enumerate}•
%ad hoc lower bounds

Muliparty?

\subsection{Poly low-error PCPs with polylog $n$ queries via modular composition}

CIRCUIT-SAT$\in \NP=PCP$.
%when $\Pj(V accepts)>\ep$, $S$ true.

Parameters
\begin{itemize}
\item
$\ep$ error probability
\item
$\Si$ alphabet size
\item
$k$ number of provers.
\end{itemize}•

%write down all questions migth ask
We require $r=|R|=O(\ln|S|)$. ALMSS give $\ep<1,k,\Si$ constant.

What other parameters possible?
\begin{itemize}
\item
$\ep\ge \rc{\poly(|S|)}$. %false no way to satisfy any questions, can just decide on own. lb unless P=NP
\item
$\Si \ge \prc{\ep}^{O\prc k}$.
\item 
$k\ge 2$.
\end{itemize}
BGLR93 conjecture: These are the only constraints. 

Results: fix $\ep=\rc{|S|}$. $\Si \ge |S|^{\Om\prc k}$, $k\ge 2$.
\begin{enumerate}
\item
Previously, $\Si>e^{|S|}$ or $k>\ln(|S|)^{\Om(1)}$.
\item
Our result: $k=\ln\ln (|S|)^{O(1)}$.
$\Si=|S|^{1/\poly\log\log(|S|)}$. (Exponential improvement.)
\end{enumerate}
%force new techniques.

Derandomized repetition: $\Si$ constant, $k=\ln|S|$. AS98, RS97, DFKRS99, 11: for any $\de\in(0,1)$, $\Si = |S|^{1/\ln^\de|S|}$, $k=O\prc{\de}\ln^\de|S|$.
%nonconstant. logloglog|S|/\log\log|S|.

PCP's are made of 
\begin{itemize}
\item
components: using locally-decodable codes, sum-check, etc.
%known since AS93, RS98

Hardwired mess?
\item
composition: recursive application of components

Principal idea from AS92.

PCP of proximity: modular composition for large error parameters.

AS93, RS97: local list-decoding of codes.

Local-reader, LDRC (for small error)

DH13: list-decoding PCP's, modular compositions for small error. Gets stuck for many compositions.
%increase exp?'
%get rid of lists

Distributional decodale PCP's, handles non-constant number of compositions.

How does modular composition work? Projection PCPs. Special $A$, other $B_i$. 

$V$ verifies
\begin{itemize}
\item
$A$'s answer, $\Phi(\ga)$ for some $\Phi$
\item
consistency: $\forall i,p_i(\ga)=\be_i$.
\end{itemize}•
$\ga$ has large alphabet... Try to decrease using composition.

$V'$ access other subroutines $A',B_i'$. ???

Attempt 2: Find $f_i(x)$, for some $x$ such that $\Phi(x)$.

Attempt 3... MORE SNOWMEN with scrolls and laser powers
\end{itemize}
We seem to have found more correct way to compose PCPs. 


\subsection{List decoding radius of Reed-Muller codes over small fields}

Codes:
\begin{itemize}
\item
rate (how many codewords?)
\item
minimum (pairwise) distance $d_{\min}$.
\end{itemize}
%pack points far apart.

List decodability.

Reed-Muller codes. points are $\F^n\to \F$, codewords are $P,\deg(P)\le d$.
%distance is $\Pj\ne$.
Schwartz-Zippel: $\de_{\min}=1-\fc dp$.

List-decoding radius is $\max r_0$ such that in a ball of radius $r_0\ep$, the number of codewords is independent of $n$.
%%?exp in n^d. dep on n
Look at a ball of radius $1-\fc dp$. How many codewords? $p^n$. Look at polynomials $P(x)=(L(x)-1)\cdots (L(x)-d)$. $p^n$ such functions. Does not rule out list decoding radius being $1-\fc dp$.

Past work: List decoding, small fields.
\begin{enumerate}
\item
Goldreich-Levin: $p=2,d=1$. Linear polys over $\F_2$.
\item
Goldreich, Rubinfeld, Sudan, 1995. 
\item
Gopalan, Klivans, Zuckerman, 2008. For any fixed degree, indpendent of $n$. All $d$, $p=2$. Specific to $p=2$. $c(d,p,\ep)$ codewords.
Conjectured true for all $p$.

Gave black box algorithm: combo bound gives algorithmic construction.
\item
Gopalan showed this for $d=2$ and any $p$.
\end{enumerate}

Beyond $\de(d,p)$. Fix $e<d$. Ball of radius $\de(e,p)-\ep = 1-e/p-\ep$. Number of codewords $>e^{n^{d-e}}$.

$p=2$: tight.

We extend it to all $p$.

Corollary: weight distribution. Number of codewords of weight $\le 1-e/p-\ep$ is $e^{\Te_{p,d,\ep}(n^{d-e})}$.
%how and when number of poly jum.

Given $\Pj(P(x)=g(x))>\fc dp+\ep$.
%cannot be too many structures.

Simple cases: $g(x)$ constant implies $P(x)$ is constant.
$g(x)=g(x_1,\ldots, x_c)$ implies $P(x)=P(x_{[1,c]})$. Only $p^{p^c}$ independent of $n$. (Key observation.)

Given $g:F^n\to F$. Approximate.
\begin{enumerate}
\item
Weak regularity lemma: get low complexity proxy $g'$ for $g$ mde of few low degree polynomials.

if $\Pj(P=g)>\fc dp+\ep$, ithen $\Pj[P=T_P(P_{[1,c]})]>\fc dp=\eph$. if actually variables, then done. Here, low-degree polynomials. 
\item
Any $f$ close to $g, g'$ is a composition of few low degree polynomials.

So we need higher-order Fourier analysis to show independent enough.

$P(x)=T(P_1,\ldots, P_c)$. Number of $P$'s $<c(p,d,\ep)$. $P_i$'s regular by higher-order FA.
\end{enumerate}•
Follow-up: extend to nonprime fields.

With Lovett, extend to large fields.

List-decoding radius is min distance.

%degree scale with vars
%dependence on $d$ deep. large degree, alg geom.

\subsection{Cpacity of causal binary channels}

Alice transmits a message to Bob. Eve can corrupt the message. Several models:
\begin{enumerate}
\item
random noise (Shannon)
\item
worst case (Hamming): best bounds: Gilbert-Varzamov, etc.
\item
(weaker channel model) list decoding, weakened reconstruction goal
\item
causal adversary: between random noise and worst case adversaries. Even decides how to corrupt based only on previously transmitted messages. 
\end{enumerate}
We give tighter bounds for causal adversary.
%Upper $1-2p$. Lower bound>

Upper bound: need adversarial strategy for Eve. Babble-and-push attack. 

\begin{itemize}
\item
Babbling phase: behave like random noise. Randomly tamper $np$ bits. 
\item
Push phase: construct set of codewords based on coruped bits transmitted so far. %freqs close.

Sleect one codeword from set and push transmitted codeword towards selected one. With Plotkin's bound, the tampered word lies midway between transmitted and selected word. Both of them look the same to Bob.
%negative list-decoding condition (Shannon capacity less than number of messages) and energy-bounding condition (). 
\end{itemize}

Lower bound:

\begin{enumerate}
\item
Encoder: deterministic codes.
%Codeword ebing transmitted.

After $\approx nR$ bits Eve can learn the codeword.
\item
Encoder: cdoes with 1-time randomness. Messages mapped to different codewords with different probabilities. After $n(R+\ep)$ bits can learn the codeword. 

Distribute randomness into codewords independently. Keep tossing the coin and adding randomness every other time.
\item
Encoder: concatenated codes with privte randomness. 

Behaviors of Eve: enclosed by curves: use at beginning vs. end. Bob's guess of Eve's behavior.
%if consistency check fail, increase and repeat.

Intersect. Possible successful decoding points.
\item
Decoding: list-decoding and unique decoding. Obtain list of messages, wirte up encoings. Consistency checking. Two words consistent if differ in limited number of places.

Look at Hamming balls $r=\fc{n-1}4$. If in intersection or non, then fails consistency checking. Increase $t$ to next position.

If in exactly 1 ball, with high probability can decode. %C\ge \min_p\rc n(1-H\pf{np}t 
\end{enumerate}
Where is causality used? Eve does not know what suffix of the transmitted codewordl. There is sufficient randomness i the suffixes of codewords by design. Eve cannot push all the balls close to each other.

\subsection{Linear-sized Spectral sparsification in almost quadratic time and regret minimization beyond matrix multiplicative updates}

Given $G$ we want to turn it into sparser $\wt G$ with $L_G=\sum_{e\in E}w_eL_e$, $L_{\wt G}=\sum_{e\in E} s_eL_e$, $|\wt E|\le \wt O(n)$. Want 
\[
(1-\ep)L_G\preceq L_G\preceq (1+\ep)L_G.
\]
More generally, turn $\sum_{e\in E}v_ev_e^T=1$ into $\sum s_i v_iv_i^T$.

History
\begin{enumerate}
\item
Subdivide graph into expanders and repeatedly sample. 
\item
Sample by effective resistance, matrix concentration bounds
\item
$O\pf{n}{\ep^2}$, $\wt O(n^4)$ running time. Novel ad-hoc potential function.
\item
$O\pf{n}{\ep^2}$, $\wt O(n^{2.001})$ running time. Novel ad-hoc potential function.
Optimization techniques: Regularization/smoothing.
\end{enumerate}•
We show a family of algorithms parametrized by $q\ge 2$: $O(\sqrt q\fc{n}{\ep^2})$, $\wt O(n^{2+\rc q})$. Construct sparsifier iteration by iteration. 

\[
A^{(t)}=A^{(t-1)}+s_{e_t}v_{e_t}v_{e_t}^T.
\]

%General view 
Main contributions: Unifying framework based on optimization techniques subsumes previous potential techniques. 
\begin{enumerate}
\item
Dual: online optimization: follow the regularized learder, mirror descent.
\item
Primal: non-smooth optimization: smoothing, coordinate descent.
\end{enumerate}

Computational speed-up.

Sparsification vs. online optimization.
Given an algorithm $X^{(t)}\in \De_n$, adversary $M^{(t)}\in S^n(\R)$ loss matrix, loss suffered is $M^{(t)}\bullet X^{(t)}$ matrix inner product. Repeat $T$ times. Minimize $R_T=\sum_{t=1}^T M^{(t)}\bullet X^{(t)}-\min_{U\in \De_n}\sum^T M^{(t)}\bullet U$.
First is loss of algorithm, second is loss of best desnity matrix, $\la_{\min}(\sum^T M^{(t)})$

MMWU algorithm: trivial generalization to SDP world
\[
X^{(t)} \propto e^{-\al\sum_{i=1}^{t-1} M^{(i)}}.
\]
General version of MMWU regret bound: 
\[
R_T\le \fc{\ln n}{\al}+\al\sum_{t=1}^T (M^{(t)})^2\bullet X^{(t)}
\]
fixed diamter term, per-iteration widgdth terms.

Larger $\al$, Explore space faster; more apt to be fooled by adversry.


\[
\la_{\min}\pa{\sum^T M^{(t)}} \ge \sum M\bullet X- ...
\]
Idea: use regret bound to lower ound $\la_{\min}$. Similar for $\la_{\max}$. 

Choose edges and weights to make $\sum^T s_ev_ev_e^T\bullet X$ large.

Get Spielman-Srivastava result. Pick $e$ with $p_e=\fc{\ve{v_e}^2}{n}$. Ensures $s_evv^T$ is unbiased estimate.
\[
\E\la_{\min}\ge 1-\fc{\al n}{T}-\fc{\ln n}\al
\]
$\al=\fc{2\ln n}{\ep}$, $T=\fc{4n\ln n}{\ep^2}$. 
%Sampling $e_t$ 


Can we do better? MMWU is instantiation of larger class of online algorithms: follow the regularized leader. Each FTRL gives different tradeoff.

$MD_{\rc2}$ algorithm. Instead of exponential, $X^{(t)}=(cI-A^{(t)})^{-2}$, for $c>0$ such that $X^{(t)}\in \De_n$. Modify edge choise. $\ge 1-\fc{\al\sqrt n}{T}-\fc{\sqrt n}{\al}$. Slower at moving around. $T=O\pf{n}{\ep^2}$. 

$p_e=\fc{\ve{v_e}\ve{v_e}_{X^{(t)}}}{\sqrt n}$. %much slower.

Primal view: smoothing and regularization.
\begin{itemize}
\item
Design potential function catpure eigenvalue of $A$: $\Phi(A^{(t)})\approx \la_{\min}$. 
\item
Perform coordinate descent in edge basis.
\item  Converges fast if $\Phi$ has Lipschitz gradient (smooth).
\item
Note $\la_{\min}=\min_{X\in \De_n}^{(t)}\bullet X$ not smooth So regularize: $\rc{\al}F(X)$. Regularized introduces error $\rc{\al}\De$ error. But stable under perturbation. Apply coordinate descent with regularized potential. 
$-\al\ve{s_ev_ev_e^T}_*^2$. (width term) Regret bound.
\end{itemize}•
MMWU is based on entropy regularizer $F(X)=X\bullet \ln X-I\bullet X$. Asymptotically optimal for small size steps. We base on $\sqrt {} $regularizer $\Tr(X^{\rc 2})$. 

QUestions: other applications? (Steltjes ransform in random matrix theory, multi-armed bandit) other usedful regularizer? (ex. logdet regularizer)