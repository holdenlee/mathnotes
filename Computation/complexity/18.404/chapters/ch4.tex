\lecture{Tue. 9/18/12}

Last time we talked about
\begin{itemize}
\item
Regular expressions$\leftarrow$ DFA
\item
Pumping lemma
\end{itemize}
Today we'll talk about CFG's, CFL's, and PDA's.

Homework 1 is due Thursday.
\begin{itemize}
\item
Use separate sheets.
\item
No bibles, online solutions, etc.
\item
Office hours
\begin{itemize}
\item
Michael Sipser: Monday 3-5
\item
Zack: Tuesday 4-6 32-6598
\item
Alex: Wednesday 2-4 32-6604
\end{itemize}
\end{itemize}
%hw is popular topic of discussion at oh.
\subsection*{Homework hints}
\noindent
\textbf{Problem 2} (1.67, rotational closure):

If $A$ is a language, $w=xy\in A$, then put $yx\in RC(A)$. Prove that if $A$ is regular, then $RC(A)$ is also regular. If $M$ is a finite automaton and $L(M)=A$, then you need to come up with a finite automaton that recognizes the rotational closure of $A$. The new automaton must be able to deal with inputs that look like $yx$.

Don't just try to twiddle $M$.

If you were pretending to be a finite automaton yourself, how you would go about determine if a string is in the rotational closure of the original language?

Recall, for $yx$ to be in the rotational closure, the original automaton should accept $xy$. How would you run the original automaton to see whether the string is a rearranged input of something the original automaton would have accepted?

If only you could see $x$ in advance, you would know what state you get to after running $y$! Then you could start there, run $y$, then run $x$, and see if you get back where you started. But you have to pretend to be a finite automaton, so you can't see $x$ first.

The magic of nondeterminism will be helpful here! You could guess all possible starting states, and see if any guess results in accept. ``Guess and check" is a typical pattern in nondeterminism.\\

\noindent
\textbf{Problem 3} (1.45, $A/B$ is regular, where $A$ is regular and $B$ is any):
We get $A/B$ as follows: start with $A$ and remove all the endings that can be in $B$. In other words, $A/B$ consists of all strings such that if you stick in some member of $B$, you get a member of $A$. 

Note you don't necessarily have a finite automaton for $B$ because $B$ is not necessarily regular! This might be surprising. Think about how you would simulate a machine for $A/B$. If a string leads to one of the original accepting states, you might want accept it early. You don't want to see rest of string if the rest of the string is in $B$.

Looked at the right way, the solution is transparent and short.

Again, think of what you would do if you were given the input and wanted to test if it was in the language.\\

\noindent\textbf{Problem 4} (1.46d): When you're using the pumping lemma, you have to be very careful. The language you're supposed to work with consists of strings $wtw$ where $|w|,|t|\ge 1$. For example, $0001000$ is in the languge, because we can let \[\underbrace{000}_w\underbrace{1}_t\underbrace{000}_w.\] 
If we add another 0 to the front, it's tempting to say we're not out of the language. {\it But we're still in the language} because we can write
\[
\underbrace{000}_w\underbrace{01}_t\underbrace{000}_w.
\]
You don't get to say what $w$ and $t$ are. As long as there is some way of choosing $w$ and $t$, it's in the language.
%what keep track of if operate nondeterministically 
\subsection{Context-Free Grammars}
We now talk about more powerful ways of describing languages than finite automata: context-free grammars and pushdown automata. Context free grammars and pushdown automata have practical applications: we can use them to design controllers, and we can use them to describe languages, both natural languages and programming languages.

\subsubsection{Example}

We introduce context-free grammars with an example.

A context-free grammar has variables, terminals, and rules (or predictions).
\begin{gather*}
S\to OS1\\
S\to R\\
R\to \ep
\end{gather*}
In the above, the three statement above are \textbf{rules}, $R$ is a variable, and the $1$ at the end of $OS1$ is a terminal. The symbols on the left hand side are \textbf{variables}. The symbols that only appear on the right hand side are called \textbf{terminals}.

We use a grammar to generate a language as follows. Start out with the symbol on the LHS of the topmost rule, $S$ here. The rules represent possibilities for substitution. Look for a variable in our current expression that appears on the LHS of a rule, substitute it with the RHS. For instance, in the following we replace each bold string by the string that is in blue in the next step.
\begin{gather*}
\bf{S}\\
{\color{blue} 0{\bf S}1}\\
0{\color{blue} 0{\bf S}1}1\\
00{\color{blue} {\bf R}}11\\
00{\color{blue} {\bf \ep}}11\\
0011.
\end{gather*}
When we have a string with only terminal symbols, we declare that string to be in the langage of $G$. So here
\[
0011\in L(G).
\]

\prbbox{What is the language of $G$?}
\vskip0.15in

We can repeat the first rule until we get tired, and then terminate by the 2nd and 3rd rules. We find that 
\[L(G)=\set{0^k1^k}{k\ge 0}.\]
The typical shorthand combines all rules that have the same left hand side into a single line, using the symbol $|$ to mean ``or." So we can rewrite our example as
\begin{align*}
S&\to OS1| R\\
R& \to \ep.
\end{align*}
%$|$ is shorthand for ``or." (It is a reserved symbol.)


\begin{ex}
Define $G_2$ to be the grammar
\begin{align*}
E&\to E+T|T\\
T& \to T\times F|F\\
F&\to (E)|a.
\end{align*}
The variables are
\[
(V)=\{E,T,F\};
\]
the terminals are
\[
(\Si)=\{a,+,\times, (,)\}.
\]
(We think of these as symbols.)
This grammar represents arithmetical expressions in $a$ using $+,\times$, and parentheses; for instance, $(a+a)\times a\in L(G)$. 

This might appear as part of a larger grammar of a programming language.

Here is the parse tree for $(a+a)\times a$.

%\tikzstyle{state}=[circle,draw,inner sep=0pt,minimum size=6mm]
%\tikzstyle{accept}=[circle,draw,inner sep=0pt,minimum size=7.5mm]
\begin{center}
\begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=1cm,semithick]
\node (0) at (0,0) {$E$};
\node (1) at (0,-1) {$T$};
\node (2a) at (-1,-2) {$T$};
\node (2b) at (0,-2)  {$\times$};
\node (2c) at (1,-2) {$F$};
\node (3a) at (-1,-3) {$F$};
\node (3b) at (1,-3) {$a$};
\node (4a) at (-2,-4) {$($};
\node (4b) at (-1,-4) {$E$};
\node (4c) at (0,-4) {$)$};
\node (5a) at (-2,-5) {$E$};
\node (5b) at (-1,-5) {$+$};
\node (5c) at (0,-5) {$T$};
\node (6a) at (-2,-6) {$T$};
\node (6b) at (0,-6) {$F$};
\node (7a) at (-2,-7) {$F$};
\node (7b) at (0,-7) {$a$};
\node (8) at (-2,-8) {$a$};
\path (0) edge node {} (1);
\path (1) edge node {} (2a);
\path (1) edge node {} (2b);
\path (1) edge node {} (2c);
\path (2a) edge node {} (3a);
\path (2c) edge node {} (3b);
\path (3a) edge node {} (4a);
\path (3a) edge node {} (4b);
\path (3a) edge node {} (4c);
\path (4b) edge node {} (5a);
\path (4b) edge node {} (5b);
\path (4b) edge node {} (5c);
\path (5a) edge node {} (6a);
\path (5c) edge node {} (6b);
\path (6a) edge node {} (7a);
\path (6b) edge node {} (7b);
\path (7a) edge node {} (8);
\end{tikzpicture}
\end{center}
A derivation is a list of steps in linear form: When $U,V\in (V\cup \Si)^*$, we write $U\implies V$ is we get to $v$ from $u$ in one substitution. For instance we write $F\times F\implies (E)\times F$.

We write $u\stackrel{*}{\implies} v$ if we can get from $u$ to $v$ in 0, 1, or more substitution steps.
\end{ex}

\subsubsection{Formal definition}

We now give a formal definition, just like we did for a finite automaton.
\begin{df}
A \textbf{context-free grammar (CFG)}  is $G(V,\Si, S,R)$ where
\begin{itemize}
\item
$V$ is the set of \textbf{variables},
\item $\Si$ is the set of \textbf{terminals},
\item $S\in V$ is the \textbf{start variable},
and 
\item $R$ is the set of \textbf{rules}, in the form
\[
\text{variable }\to \text{ string of variable and terminals.}
\]
\end{itemize}
We say $S$ \textbf{derives} $w$ if we can repeatedly make substitutions according to the rules to get from $S$ to $w$. We write a derivation as 
\[
S\implies u_1\implies u_2\implies \cdots \implies u_{\ell}\implies w,\qquad S\stackrel{*}{\implies} w.
\]
($w$ only has terminals, but the other strings have variables too.)
We say that $G$ recognizes the language
\[
L(G)=\set{w\in \Si^*}{S\stackrel{*}{\implies}w}.
\] 
\end{df}

There is a natural correspondence between a derivation and a parse tree. Parse tree may be more relevant to a particular applications.

Note $a+a\times a\in L(G_2)$. Take a look back at the parse tree for $a+a\times a$. Reading it from the bottom up, the parse tree first groups $a\times a$ into a subtree, and {\it then} puts in the $\times$. %The only derivation you can get
There is no way to put the $+$ first, unless we put in parentheses.

This is important in a programming language! %Theoretical feature worth knowing. 
Sometimes we can have multiple parse strings for the same string---an undesirable feature in general. That means we have two different interpretations for a particular string, that can give rise to two different semantic meanings. In a programming language, we do {\it not} want two different meanings for the same expression.
% if have an ambiguous grammar. 
\begin{df}
A string is derived \textbf{ambiguously} if it has two different parse trees. A  grammar or language is \textbf{ambiguous} if some string can be derived ambiguously.
\end{df}
We won't discuss this further, but look at the section in the book for more.

\subsubsection{Why care?}
To describe a programming language in formal way, we can write it down in terms of a grammar. %If statement has the following possibilities, can describe using rules of a grammar.  
We can specify the whole syntax of the any programming language with context-free grammars. If we understand grammars well enough, we can generate a parser---the part of a compiler which will take the grammar representing the program, process a program, and group the pieces of code into recognizable expressions. The parser would then feed the expressions into another advice.

The key point is that we need to write down a grammar that represents the programming language.

Context-free grammars had their origin in the study of natural languages. For instance, $S$ might represent sentence, and we may have rules such as
\begin{align*}
S&\to\pat{noun phrase}\pat{verb phrase},\\
\pat{verb}&\to\pat{adverb}\pat{verb},\\
\pat{noun}&\to\pat{adjective}\pat{noun},
\end{align*}
and so forth. We can gain insight into the way a language works by specifying it this fashion.

This is a gross oversimplification, but both the study of programming and natural languages benefit from the study of grammars.
%step/step through code; machine code

We're going to shift gears now, and then put everything together in the next lecture.

\subsection{Pushdown automata}

Recall that we had 2 different ways for describing regular languages, using a 
\begin{itemize}
\item
{\it computational device}, a finite automaton, which recognize members of regular languages when it runs.
\item
{\it descriptive device}, a regular expression, which generates members of regular languages.
\end{itemize}
We found that finite automata and regular expressions recognize the same class of languages (Theorem~\ref{thm:regex-FA}). A CFG is a descriptive device, like a regular expression. We will find a computational device that recognizes the same languages as CFG's. First, a definition.

\begin{df}
A \textbf{context-free language (CFL)} is one generated by a CFG.
\end{df}
We've already observed that there is a CFL that is not regular: we found a CFG generating the language $\{0^k1^k\}$, which is not regular. We will show in fact that the CFL's include all regular languages. More on this later.

\subsection{Comparing pushdown and finite automata}
%Context-free languages.
We now introduce a computational device that recognizes exactly the context-free languages: a pushdown automaton (PDA).
%guesswork
%given a grammar and string. How can I tell whether the grammar generates the string or not? We will do that kind of thing. First we'll show there is an algorithm; later on we'll see there is an efficient algorithm. This is a practical algorithm. We'll see an algorithm works in principle, and moderately efficiently.
A pushdown automaton is like a finite automaton with a extra feature called a {\it stack}.
%
%Imagine a finite automaton; we'll show 
%Ex

\ig{4-0}{1}

In a finite automaton, we have a finite control, i.e. different states with rules of how to transition between them. We draw a schematic version of a finite automaton, as above. A head starts at the beginning of the input string, and at each step, it moves to the next symbol to the right.

A pushdown automata has an extra feature. It is allowed to write symbols on the stack, not just read symbols.

\ig{4-1}{1}

However, there are some limitations. A pushdown automata can only look at the topmost symbol of a stack. When it writes a symbol to the stack, what's presently there gets pushed down, like a stack of plates in a cafeteria. When reading, the reverse happens. In one step the automata can only pop off the topmost symbol; then the remaining symbols all move back up.
We use the following terminology:
\begin{itemize}
\item
\textbf{push} means ``add to stack," and
\item
\textbf{pop} means ``read and remove from stack."
\end{itemize}

%Tape where only have access at end. Grows and shrinks. Or push down.

When we looked at FA's, we considered {\it deterministic} and {\it nondeterministic} variants. For PDA's, we'll only consider the nondeterministic variant. A deterministic version has been studied, but in the case of pushdown automata they are not equivalent. Some languages require nondeterministic PDA's.

Deterministic pushdown automata have practical applications to programming languages, because the process of testing whether a language is valid is especially efficient if the PDA is deterministic. This is covered in the 3rd edition of the textbook.

Let's give an example.
\begin{ex}\llabel{ex:akbk}
We give a PDA for $A=\set{0^k1^k}{k\ge 0}$.

As we've said, a PDA is a device that looks like FA but also have stack can write on. Our PDA is supposed to test whether a string is in $A$.

If we used an ordinary FA, without a stack, then we're out of luck. Intuitively, a FA has finite memory, and we can't do this language with finite memory. The stack in a PDA, however, is just enough to allow us to ``remember stuff."\\

\prbbox{How would we design a PDA that recognizes $A$? (How would you use the stack?)}
\vskip0.15in

We can use the stack to {\it record information}. %, test whether input strings are in language. 
The idea is that every time we read a 0, stick a 0 in; every time we read a 1, pop it out. If the stack becomes empty and has not become empty beforehand, then we accept. The 0's match off with 1's that come later.
%in general same or diff as input symbol

We have to modify this idea a little bit, because {\it what if the 0's and 1's are out of order?} We don't want to accept strings where the 0's and 1's are out of order. 
%What if out of order? If we simply put a 0 every time and pop a 0 every time, then bigger language.
If we insist that 0's come before 1's, we need a finite control mechanism.

We have a state for reading 0's and another state when reading 1's. In the ``1" state the PDA no longer takes 0's and adds them to the stack. We see that a PDA combines the elements of FA with the power of a stack.


Now we ask: {\it how do we know when to transition from reading 0's to reading 1's?} %%%%%%%%%%%%%%
We'd like to consider different possibilities for when to transition, i.e. let several parallel threads operate independently, and if any thread gets to an accept state, then have the machine accepts the input. Hence we turn to nondeterminism: every time there's a choice, the machine splits into different machines which operate independently, each on its own stack. 

At every step when the machine is reading 0's, we give it a nondeterministic choice: in the next step the machine can continue to push 0's on the stack, or transition into reading 1's and popping 0's off the stack. 
\end{ex}

\subsubsection{Formal definition}
Let's write down the formal definition.
\begin{df}
A \textbf{pushdown automaton (PDA)} is a 6-tuple $P=(Q,\Si, \Ga, \de,q_0,F)$
where 
\begin{itemize}
\item
$Q$ are the states
\item 
$\Si$ is the input alphabet,
\item
$\Ga$ is the stack alphabet,
\item
$\de$ is the transition function,
\item
$q_0$ is the start state,
\item 
$F$ is the accept states.
\end{itemize}
Here $Q$, $\Si$, $q_0$, and $F$ are as in a finite automata, but the transition function is a function $\de:Q\times \Si_{\ep}\times\Ga_{\ep}\to \cal P(Q\times \Ga_{\ep})$ (we explain this below).
\end{df}
On first thought, we may think to define the transition function as a function 
\[\de:Q\times \Si\times \Ga\to Q\times \Ga.\]
The function takes as input 
\begin{itemize}
\item
a state in $Q$---the current state of the machine,
\item
a symbol from $\Si$---the next symbol to read, and
\item
a symbol from $\Ga$---the top-of-stack symbol.
\end{itemize}â€¢
It outputs a another state in $Q$ to transition to, and a symbol from $\Ga$---the next symbol to push on the stack.

However, we have to modify this: we want nondeterminism, so we allow the machine to transition to an entire {\it set} of possible next states and next symbols, and we represent this by having $\de$ output a {\it subset}:
\[\de:Q\times \Si\times \Ga\to \cal P(Q\times \Ga).\]
We also allow $\de$ to read an empty string, or read without popping a string on the stack, and proceed without writing a symbol, %no read, no pop.
so we actually want
\[
\de:Q\times \underbrace{\Si_{\ep}}_{\Si\cup \{\ep\}}\times \underbrace{\Ga_{\ep}}_{\Ga\cup \{\ep\}}\to \cal P(Q\times \Ga_{\ep}).
\]
%How does definition include popping. Pop it off and read it. Can't read and leave it there. Lift it off and look at it closely. If just want to read, need to write it back down.

%the stack is unlimited. nothing infinite actually happens.
%never >linear.
We'll do one more example and save proofs to next time.

\begin{ex}
Consider the language
\[
\set{ww^{\cal R}}{w\in \{0,1\}},
\]
where $\cal R$ means ``reverse the word."
This is the language of even-length palindromes such as 0110110110. A PDA recognizing this language uses nondeterminism in an essential way. We give a sketch of how to construct a PDA to recognize it. (See the book for details.)

The PDA has to answer: does the 2nd half of the word match the first half?

We should push the first half of the word on the stack. When we pop it off, the string comes out backwards, and we can match it with the second half of the word. This is exactly what we need. 

But how do we know we're at the middle? When do you shift from pushing to popping and matching?

Can we find the length of the word? No. Instead, we guess nondeterministically at every point that we're at the middle! If the word is a palindrome, one of the threads will guess correctly the middle, and the machine will accept. %Another thread will push everything along the way hoping for more to come, get to end but not in accepting state.
\end{ex}